{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ef7bb-0e31-4151-9fb0-50de1141d91e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffe1f7f4ada8c4a3160626f214fc1e36",
     "grade": false,
     "grade_id": "cell-48cccda20e73351e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Hackathon: From Raw Data to ML-Ready Dataset\n",
    "## Insight-Driven EDA and End-to-End Feature Engineering on Airbnb Data Using pandas and Plotly\n",
    "\n",
    "### What is a Hackathon?\n",
    "\n",
    "A hackathon is a fast-paced, collaborative event where participants use data and technology to solve a real problem end-to-end.  \n",
    "In this hackathon, you will work with a **real-world Airbnb dataset** and complete two interconnected goals:\n",
    "\n",
    "- Produce a **high-quality exploratory data analysis (EDA)** using `pandas` and `plotly`, extracting meaningful insights, trends, and signals from the data.  \n",
    "- Design and deliver a **clean, feature-rich, ML-ready dataset** that will serve as the foundation for a follow-up hackathon focused on building and evaluating machine learning models.\n",
    "\n",
    "Your task is to **get the most out of the data**: uncover structure and patterns through EDA, and engineer informative features (numerical, categorical, temporal, textual (TF–IDF), and optionally image-based) to maximize the predictive power of the final dataset.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>About the Dataset</b>\n",
    "\n",
    "<u>Context</u>\n",
    "\n",
    "The data comes from <a href=\"https://insideairbnb.com/get-the-data/\">Inside Airbnb</a>, an open project that publishes detailed, regularly updated datasets for cities around the world.  \n",
    "Each city provides three main CSV files:\n",
    "\n",
    "- <b>listings.csv</b> — property characteristics, host profiles, descriptions, amenities, etc.  \n",
    "- <b>calendar.csv</b> — daily availability and pricing information for each listing.  \n",
    "- <b>reviews.csv</b> — guest feedback and textual reviews.\n",
    "\n",
    "These datasets offer a rich view of the short-term rental market, including availability patterns, pricing behavior, host attributes, and guest sentiment.  \n",
    "\n",
    "<u>Inspiration</u>\n",
    "\n",
    "Your ultimate objective is to create a dataset suitable for training a machine learning model that predicts whether a specific Airbnb listing will be <b>available on a given date</b>, using property attributes, review information, and host characteristics.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Task</b>\n",
    "\n",
    "Using one city of your choice from Inside Airbnb, create an end-to-end pipeline that:\n",
    "\n",
    "1. Loads and explores the raw data (EDA).  \n",
    "2. Engineers features (numerical, categorical, temporal, textual TF–IDF, etc.).  \n",
    "3. Builds a unified ML-ready dataset.  \n",
    "\n",
    "Please remember to add comments explaining your decisions. Comments help us understand your thought process and ensure accurate evaluation of your work. This assignment requires code-based solutions—**manually calculated or hard-coded results will not be accepted**. Thoughtful comments and visualizations are encouraged and will be highly valued.\n",
    "\n",
    "- Write your solution directly in this notebook, modifying it as needed.\n",
    "- Once completed, submit the notebook in **.ipynb** format via Moodle.\n",
    "    \n",
    "<b>Collaboration Requirement: Git & GitHub</b>\n",
    "\n",
    "You must collaborate with your team using a **shared GitHub repository**.  \n",
    "Your use of Git is part of the evaluation. We will specifically look at:\n",
    "\n",
    "- Commit quality (clear messages, meaningful steps).  \n",
    "- Balanced participation across team members.  \n",
    "- Use of branches.  \n",
    "- Ability to resolve merge conflicts appropriately.  \n",
    "- A clean, readable project history that reflects real collaboration.\n",
    "\n",
    "Good Git practice is **part of your grade**, not optional.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    "    You are free to add as many cells as you wish as long as you leave untouched the first one.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Hints</b>\n",
    "\n",
    "- Text columns often carry substantial predictive power, use text-vectorization methods to extract meaningful features.  \n",
    "- Make sure all columns use appropriate data types (categorical, numeric, datetime, boolean). Correct dtypes help prevent subtle bugs and improve performance.  \n",
    "- Feel free to enrich the dataset with any additional information you consider useful: engineered features, external data, derived temporal features, etc.  \n",
    "- If the dataset is too large for your computer, use <code>.sample()</code> to work with a subset while preserving the logic of your pipeline.  \n",
    "- Plotly offers a wide variety of powerful visualizations, experiment creatively, but always begin with a clear analytical question: *What insight am I trying to uncover with this plot?*\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Submission Deadline:</b> Wednesday, December 3rd, 12:00\n",
    "\n",
    "Start with a simple, working pipeline.  \n",
    "Do not over-complicate your code too much. Start with a simple working solution and refine it if you have time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "You may add as many cells as you want, but the **first cell must remain exactly as provided**. Do not edit, move, or delete it under any circumstances.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da41098",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8431230dc8647851888c39d82eb7078d",
     "grade": true,
     "grade_id": "ex1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84467c34-1fa2-4f03-b7ba-e216836ff6b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bce797b7aa5f22189e671fd29fa5841",
     "grade": false,
     "grade_id": "cell-140b4c12d85796ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Team Information\n",
    "\n",
    "Fill in the information below.  \n",
    "All fields are **mandatory**.\n",
    "\n",
    "- **GitHub Repository URL**: Paste the link to the team repo you will use for collaboration.\n",
    "- **Team Members**: List all student names (and emails or IDs if required).\n",
    "\n",
    "Do not modify the section title.  \n",
    "Do not remove this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a430c-3e17-42e4-9b63-3bafd596c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Team Information (Mandatory) ===\n",
    "# Fill in the fields below.\n",
    "\n",
    "GITHUB_REPO = \"\"       # e.g. \"https://github.com/myteam/airbnb-hackathon\"\n",
    "TEAM_MEMBERS = [\n",
    "    # \"Full Name 1\",\n",
    "    # \"Full Name 2\",\n",
    "    # \"Full Name 3\",\n",
    "]\n",
    "\n",
    "GITHUB_REPO, TEAM_MEMBERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5723a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling process...\n",
      "Selected 1000 unique listings.\n",
      "\n",
      "--- SAMPLING COMPLETE ---\n",
      "Original Listings: (3679, 79) -> Sample: (1000, 79)\n",
      "Original Calendar: (1342835, 7) -> Sample: (365000, 7)\n",
      "Original Reviews:  (83000, 6) -> Sample: (21615, 6)\n",
      "\n",
      "Files saved in 'data/': listings_sample.csv, calendar_sample.csv, reviews_sample.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    " \"\n",
    "# --- CONFIGURATION ---\n",
    "DATA_FOLDER = \"data\"        # Folder where your raw csv.gz files are located\n",
    "N_LISTINGS = 1000           # Number of unique properties to sample\n",
    "SEED = 42                   # Seed for reproducibility (so your team gets the same sample)\n",
    "\n",
    "print(\"Starting sampling process...\")\n",
    "\n",
    "# 1. Load Listings (The \"Parent\" Table)\n",
    "# We start here because we need to select specific IDs first to keep consistency.\n",
    "listings_path = os.path.join(DATA_FOLDER, \"listings.csv.gz\")\n",
    "df_listings = pd.read_csv(listings_path, compression=\"gzip\")\n",
    "\n",
    "# 2. Create the Sample of Listings\n",
    "# We verify if we have enough data before sampling\n",
    "if len(df_listings) > N_LISTINGS:\n",
    "    df_listings_sample = df_listings.sample(n=N_LISTINGS, random_state=SEED)\n",
    "else:\n",
    "    # If the dataset is smaller than the requested sample, take everything\n",
    "    df_listings_sample = df_listings.copy()\n",
    "    \n",
    "# Get the list of IDs we selected. This is the key to filter the other files.\n",
    "selected_ids = df_listings_sample['id'].unique()\n",
    "\n",
    "print(f\"Selected {len(selected_ids)} unique listings.\")\n",
    "\n",
    "# 3. Load and Filter Calendar (The \"Target\" Table)\n",
    "# This file is usually huge, so we filter it to keep only rows for our selected IDs.\n",
    "calendar_path = os.path.join(DATA_FOLDER, \"calendar.csv.gz\")\n",
    "df_calendar_raw = pd.read_csv(calendar_path, compression=\"gzip\")\n",
    "df_calendar_sample = df_calendar_raw[df_calendar_raw['listing_id'].isin(selected_ids)].copy()\n",
    "\n",
    "# 4. Load and Filter Reviews (Text Data)\n",
    "# Same logic: only keep reviews that belong to the selected houses.\n",
    "reviews_path = os.path.join(DATA_FOLDER, \"reviews.csv.gz\")\n",
    "df_reviews_raw = pd.read_csv(reviews_path, compression=\"gzip\")\n",
    "df_reviews_sample = df_reviews_raw[df_reviews_raw['listing_id'].isin(selected_ids)].copy()\n",
    "\n",
    "# 5. Save the Samples\n",
    "# We save them as new files so you don't have to run this heavy process again.\n",
    "df_listings_sample.to_csv(os.path.join(DATA_FOLDER, \"listings_sample.csv\"), index=False)\n",
    "df_calendar_sample.to_csv(os.path.join(DATA_FOLDER, \"calendar_sample.csv\"), index=False)\n",
    "df_reviews_sample.to_csv(os.path.join(DATA_FOLDER, \"reviews_sample.csv\"), index=False)\n",
    "\n",
    "# 6. Verification Output\n",
    "print(\"\\n--- SAMPLING COMPLETE ---\")\n",
    "print(f\"Original Listings: {df_listings.shape} -> Sample: {df_listings_sample.shape}\")\n",
    "print(f\"Original Calendar: {df_calendar_raw.shape} -> Sample: {df_calendar_sample.shape}\")\n",
    "print(f\"Original Reviews:  {df_reviews_raw.shape} -> Sample: {df_reviews_sample.shape}\")\n",
    "print(f\"\\nFiles saved in '{DATA_FOLDER}/': listings_sample.csv, calendar_sample.csv, reviews_sample.csv\")\n",
    " \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4befb12",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\sklearn\\utils\\fixes.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\scipy\\stats\\__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Import unused here but needs to stay until end of deprecation periode\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# See https://github.com/scipy/scipy/issues/15765#issuecomment-1875564522\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_mstats_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _find_repeats, theilslopes, siegelslopes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\scipy\\stats\\distributions.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_distn_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_continuous_distns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:11491\u001b[39m\n\u001b[32m  11487\u001b[39m         y[mask] = -y[mask]\n\u001b[32m  11488\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[32m> \u001b[39m\u001b[32m11491\u001b[39m gennorm = \u001b[43mgennorm_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgennorm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m  11494\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhalfgennorm_gen\u001b[39;00m(rv_continuous):\n\u001b[32m  11495\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"The upper half of a generalized normal continuous random variable.\u001b[39;00m\n\u001b[32m  11496\u001b[39m \n\u001b[32m  11497\u001b[39m \u001b[33;03m    %(before_notes)s\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m  11528\u001b[39m \n\u001b[32m  11529\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1911\u001b[39m, in \u001b[36mrv_continuous.__init__\u001b[39m\u001b[34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, seed)\u001b[39m\n\u001b[32m   1909\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1910\u001b[39m     dct = \u001b[38;5;28mdict\u001b[39m(distcont)\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocdict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\63429\\micromamba\\envs\\hackathon\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:869\u001b[39m, in \u001b[36mrv_generic._construct_doc\u001b[39m\u001b[34m(self, docdict, shapes_vals)\u001b[39m\n\u001b[32m    865\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to construct docstring for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    866\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdistribution \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# correct for empty shapes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__doc__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(, \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m, )\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the folder with the sample data set\n",
    "DATA_FOLDER = \"/Users/ivan/python-hackathon/sample_data\"\n",
    "\n",
    "df_listings = pd.read_csv(f\"{DATA_FOLDER}/listings_sample.csv\")\n",
    "df_calendar = pd.read_csv(f\"{DATA_FOLDER}/calendar_sample.csv\")\n",
    "df_reviews = pd.read_csv(f\"{DATA_FOLDER}/reviews_sample.csv\")\n",
    "\n",
    "df_listings.head(), df_calendar.head(), df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings.head()\n",
    "df_listings.tail()\n",
    "\n",
    "df_calendar.head()\n",
    "df_calendar.tail()\n",
    "\n",
    "df_reviews.head()\n",
    "df_reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1256678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_listings.info())\n",
    "print(df_calendar.info())\n",
    "print(df_reviews.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1025a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listings missing values:\")\n",
    "print(df_listings.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "print(\"\\nCalendar missing values:\")\n",
    "print(df_calendar.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "print(\"\\nReviews missing values:\")\n",
    "print(df_reviews.isna().sum().sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c282a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Keys\n",
    "listings_keys = [col for col in df_listings.columns if \"id\" in col.lower()]\n",
    "calendar_keys = [col for col in df_calendar.columns if \"id\" in col.lower()]\n",
    "review_keys   = [col for col in df_reviews.columns if \"id\" in col.lower()]\n",
    "\n",
    "print(\"Listing ID columns:\", listings_keys)\n",
    "print(\"Calendar ID columns:\", calendar_keys)\n",
    "print(\"Review ID columns:\", review_keys)\n",
    "\n",
    "#identify date range in calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Keys\n",
    "listings_keys = [col for col in df_listings.columns if \"id\" in col.lower()]\n",
    "calendar_keys = [col for col in df_calendar.columns if \"id\" in col.lower()]\n",
    "review_keys   = [col for col in df_reviews.columns if \"id\" in col.lower()]\n",
    "\n",
    "print(\"Listing ID columns:\", listings_keys)\n",
    "print(\"Calendar ID columns:\", calendar_keys)\n",
    "print(\"Review ID columns:\", review_keys)\n",
    "\n",
    "#identify date range in calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ad726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar[\"available_flag\"] = df_calendar[\"available\"].map({\"t\": 1, \"f\": 0})\n",
    "\n",
    "df_calendar[\"available_flag\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "availability_by_day = (\n",
    "    df_calendar.groupby(\"date\")[\"available_flag\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    availability_by_day,\n",
    "    x=\"date\",\n",
    "    y=\"available_flag\",\n",
    "    title=\"Daily Availability Rate Over Time\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_listings = (\n",
    "    df_calendar.groupby(\"date\")[\"listing_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"listing_id\": \"num_active_listings\"})\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    active_listings,\n",
    "    x=\"date\",\n",
    "    y=\"num_active_listings\",\n",
    "    title=\"Number of Active Listings Over Time\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"property_type\" in df_listings.columns:\n",
    "    prop_counts = (\n",
    "        df_listings[\"property_type\"]\n",
    "        .value_counts()\n",
    "        .rename_axis(\"property_type\")\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    fig = px.bar(\n",
    "        prop_counts,\n",
    "        x=\"property_type\",\n",
    "        y=\"count\",\n",
    "        title=\"Property Type Distribution\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(\n",
    "    df_listings,\n",
    "    names=\"room_type\",\n",
    "    title=\"Room Type Breakdown\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in [\"accommodates\", \"bedrooms\", \"beds\", \"minimum_nights\"] if c in df_listings.columns]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    fig = px.histogram(df_listings, x=col, title=f\"Distribution of {col}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257aa658",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"host_is_superhost\" in df_listings.columns:\n",
    "    df_listings[\"host_is_superhost_flag\"] = df_listings[\"host_is_superhost\"].map({\"t\": 1, \"f\": 0})\n",
    "    fig = px.bar(\n",
    "        df_listings[\"host_is_superhost_flag\"].value_counts().reset_index(),\n",
    "        x=\"index\",\n",
    "        y=\"host_is_superhost_flag\",\n",
    "        title=\"Superhost Status Distribution\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if {\"latitude\", \"longitude\"}.issubset(df_listings.columns):\n",
    "    fig = px.scatter_mapbox(\n",
    "        df_listings,\n",
    "        lat=\"latitude\",\n",
    "        lon=\"longitude\",\n",
    "        zoom=10,\n",
    "        height=600,\n",
    "        title=\"Geographical Distribution of Listings\",\n",
    "        color=\"accommodates\"\n",
    "    )\n",
    "    fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"review_length\"] = df_reviews[\"comments\"].astype(str).apply(len)\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_reviews,\n",
    "    x=\"review_length\",\n",
    "    nbins=50,\n",
    "    title=\"Distribution of Review Lengths\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15507e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_listing = (\n",
    "    df_reviews.groupby(\"listing_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_reviews\")\n",
    ")\n",
    "\n",
    "fig = px.histogram(\n",
    "    reviews_per_listing,\n",
    "    x=\"num_reviews\",\n",
    "    nbins=30,\n",
    "    title=\"Number of Reviews per Listing\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118abb46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_calendar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Feature engineering\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Convert availability t/f → 1/0\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_calendar[\u001b[33m\"\u001b[39m\u001b[33mavailable_flag\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf_calendar\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mavailable\u001b[39m\u001b[33m\"\u001b[39m].map({\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m})\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Convert date to datetime if not already\u001b[39;00m\n\u001b[32m      7\u001b[39m df_calendar[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df_calendar[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_calendar' is not defined"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "\n",
    "# Convert availability t/f → 1/0\n",
    "df_calendar[\"available_flag\"] = df_calendar[\"available\"].map({\"t\": 1, \"f\": 0})\n",
    "\n",
    "# Convert date to datetime if not already\n",
    "df_calendar[\"date\"] = pd.to_datetime(df_calendar[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Basic temporal features\n",
    "df_calendar[\"day_of_week\"] = df_calendar[\"date\"].dt.dayofweek\n",
    "df_calendar[\"month\"] = df_calendar[\"date\"].dt.month\n",
    "df_calendar[\"day_of_year\"] = df_calendar[\"date\"].dt.dayofyear\n",
    "df_calendar[\"is_weekend\"] = (df_calendar[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "df_calendar.head()\n",
    "\n",
    "bool_cols = [\"host_is_superhost\", \"instant_bookable\"]\n",
    "\n",
    "for col in bool_cols:\n",
    "    if col in df_listings.columns:\n",
    "        df_listings[col + \"_flag\"] = df_listings[col].map({\"t\": 1, \"f\": 0})\n",
    "\n",
    "if \"description\" in df_listings.columns:\n",
    "    df_listings[\"description_length\"] = df_listings[\"description\"].astype(str).apply(len)\n",
    "else:\n",
    "    df_listings[\"description_length\"] = 0\n",
    "\n",
    "categorical_cols = [\n",
    "    c for c in [\"room_type\", \"property_type\", \"neighbourhood_cleansed\"]\n",
    "    if c in df_listings.columns\n",
    "]\n",
    "\n",
    "df_listings[categorical_cols].head()\n",
    "\n",
    "if \"host_id\" in df_listings.columns:\n",
    "    host_listing_counts = df_listings.groupby(\"host_id\")[\"id\"].count()\n",
    "    df_listings[\"host_total_listings\"] = df_listings[\"host_id\"].map(host_listing_counts)\n",
    "\n",
    "if \"host_since\" in df_listings.columns:\n",
    "    df_listings[\"host_since\"] = pd.to_datetime(df_listings[\"host_since\"], errors=\"coerce\")\n",
    "    df_listings[\"host_years_active\"] = (\n",
    "        (pd.Timestamp(\"today\") - df_listings[\"host_since\"]).dt.days / 365\n",
    "    ).fillna(0)\n",
    "else:\n",
    "    df_listings[\"host_years_active\"] = 0\n",
    "\n",
    "reviews_per_listing = (\n",
    "    df_reviews.groupby(\"listing_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_reviews\")\n",
    ")\n",
    "\n",
    "df_listings = df_listings.merge(\n",
    "    reviews_per_listing,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"listing_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_listings[\"num_reviews\"] = df_listings[\"num_reviews\"].fillna(0)\n",
    "\n",
    "df_reviews[\"review_length\"] = df_reviews[\"comments\"].astype(str).apply(len)\n",
    "\n",
    "avg_review_length = (\n",
    "    df_reviews.groupby(\"listing_id\")[\"review_length\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"avg_review_length\")\n",
    ")\n",
    "\n",
    "df_listings = df_listings.merge(\n",
    "    avg_review_length,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"listing_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_listings[\"avg_review_length\"] = df_listings[\"avg_review_length\"].fillna(0)\n",
    "\n",
    "df_features = df_calendar.merge(\n",
    "    df_listings,\n",
    "    left_on=\"listing_id\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "feature_cols = (\n",
    "    numeric_cols +\n",
    "    [\"host_total_listings\", \"host_years_active\", \"description_length\",\n",
    "     \"num_reviews\", \"avg_review_length\",\n",
    "     \"host_is_superhost_flag\", \"instant_bookable_flag\",\n",
    "     \"day_of_week\", \"month\", \"day_of_year\", \"is_weekend\"]\n",
    ")\n",
    "\n",
    "# Keep only columns that actually exist\n",
    "feature_cols = [c for c in feature_cols if c in df_features.columns]\n",
    "\n",
    "df_model_base = df_features[feature_cols + [\"available_flag\", \"listing_id\"]]\n",
    "df_model_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698917b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67494a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
